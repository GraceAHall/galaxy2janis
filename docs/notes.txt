


FILEIO TextRender
- @property imports has same logic for anything that just collects datatypes (workflowinput, workflowoutput, toolinput, tooloutput)


uuids
- anytime we need to refer to registered external entity
- ConnectionInputValue ->    step_uuid = WorkflowStep.uuid
- ConnectionInputValue ->    output_uuid = StepOutput.uuid
- WorkflowInputInputValue -> input_uuid = WorkflowInput.uuid

InputValue
- add UnknownInputValue



REMEMBER: Think about whether code would work on webapp

- paths module
    - refactor into module 
    - each TextEntity 

- formatting module
    - TextRenderer
        - has render() (recursive: each subentity is rendered, then this object is rendered)
        - has messages
        - has imports property? 
        - has curation_state property?
        - has confidence property?
        - can ask paths module where it will appear 
        - can ask paths module where something else will appear



FROMWDL  (WdlParser)
noteworthy components:
- translate_expr()
    - WDL ternary conditionals translated to j.If()
    - translates WDL expressions (WDL.Expr.Apply) to janis operators (j.AndOperator etc)
    - 


FROMCWL  (CWlParser)
- Tag formatting very simple, spread throughout multiple functions within parser
- Datatype formatting very simple (single function) but may be fine like this
- Regex patterns spread throughout (parse_number_from_string())
- Lexing very simple (parse_basic_expression(), parse_number_from_string())
- Parser very simple (convert_javascript_token())
- Parser returns '<expr>{token}</expr>' for anything it cant handle


============
MODULES TODO
============
- Tags                  [DONE]
- Containers            [DONE]
- Settings              [DONE]
- Galaxy Wrappers       [DONE]
- FileIO (incl paths)   [DONE]
- datatypes             [DONE]  
    # note: CWL has edam 'format' for File type. this is like specifying BAM etc.  
- remove testcmdstr references. will only be single pass of simplified <command>! 



SETUP ----> need to know the tool name (tool mode) or workflow name (workflow mode) to set outdir. 


==================
FUNCTIONALITY TODO
==================
- arrays
- configfiles
- multiple statements







Modules                 
- Datatypes             [PATIALLY DONE][shared]
- Formats               [PATIALLY DONE][shared]
- Logging               [][shared]
- Messaging             [][]

Language Specific
- Parser                []
- Lexer                 []
- Ingestion             []

Features 
- Multiple statements   []
- Configfiles           []
- Arrays                []



Entities (merged with janis_core)
- Workflow
- WorkflowInput
- Tool










Notes from BYOD meetings
- prioritise features common across languages
    - ie dont do cheetah test templating because its Galaxy specific
- 



================
Command / Script
================

Galaxy:
- cheetah pass  (escape \$ for env var)
- Bash pass 

Nextflow:
- substitute NF values pass (escape \$ for env var)
- Bash pass 

CWL:
- ah hahaha
- 








-----
paths
-----





startup: 
    - create folder structure using PathManager()
    - delete all folders if exist - start 100% fresh.
    - then do override logic: if wrappers folder exists, leave it, else create

step parsing:
    - read metadata of each step
    - 

downloading wrappers:
    - to know tool name and create a step tag
    - 
- 
- tags: settle step tags ASAP. after which, can ask where any file should read/write to/from
- 















tools
    tool
        - tool-@adf2123qd   [wrapper][optional]
        - tool.py           janis def
        - tool.log          janis log

workflows unified
    workflow
        - tools
        - wrappers
        - workflow.log
        - workflow.py

workflows stepwise
    workflow
        - steps
        - tools
        - wrappers
        - workflow.log
        - workflow.py








getting values
- from cheetah cmdstr
- from input dict

update tool inputs

getting values
- from default value

migrating values






















-----------------------------------------------

#import re
#import os





#set $labels = []
#for $x in $in.inputs
#if str($x.labels) != ''
#silent $labels.append(re.sub('[^\w\-_]', '_', str($x.labels)))
#else
#silent $labels.append(re.sub('[^\w\-_]', '_', str($x.input.element_identifier)))
#end if
#end for
#set $labels = ','.join($labels)










quast




































--labels '$labels'
-o 'outputdir'


#if $assembly.ref.use_ref == 'true'
-r '$assembly.ref.r'
#if $assembly.ref.features
--features '$assembly.ref.features'
#end if
#if $assembly.ref.operons
--operons '$assembly.ref.operons'
#end if
$assembly.ref.circos
$assembly.ref.k_mer.k_mer_stats
#if str($assembly.ref.k_mer.k_mer_stats) != ''
--k-mer-size $assembly.ref.k_mer.k_mer_size
#end if
#else if $assembly.ref.est_ref_size
--est-ref-size $assembly.ref.est_ref_size
#end if












--min-contig 500














--min-alignment 65
--min-identity 95.0
--ambiguity-usage 'one'
--ambiguity-score 0.99












#set $contig_thresholds = ','.join([x.strip() for x in str($advanced.contig_thresholds).split(',')])
--contig-thresholds '$contig_thresholds'

--extensive-mis-size 1000
--scaffold-gap-max-size 1000
--unaligned-part-size 500







#for $k in $in.inputs
'$k.input'
#end for


--threads ${GALAXY_SLOTS:-1}

&& mkdir -p '$report_html.files_path'
&& cp outputdir/*.html '$report_html.files_path'

#if ($assembly.type == 'genome' and $assembly.ref.use_ref) or ($assembly.type == 'metagenome' and $assembly.ref.origin != 'none')
&& cp -R outputdir/icarus_viewers '$report_html.files_path'
#end if











repeat encountered
 cp -R outputdir/icarus_viewers '$report_html.files_path'
#end if





-----------------------------------------------


class BaseWorkflowDefinitionWriter(WorkflowDefinitionWriter):
    def __init__(self, esettings: WorkflowExeSettings, workflow: Workflow):
        self.esettings = esettings
        self.workflow = workflow
        self.ignore_defaults: bool = False 
        # TODO NOTE ^^^ SHOULD BE CLI RUNTIME SETTING

    def write(self) -> None:
        self.write_header()
        self.write_imports()
        self.write_metadata()
        self.write_declaration()
        self.write_inputs()
        self.write_steps()
        self.write_outputs()




imports 
variable 
    - common imports
    - either tool imports or step imports





























Things which get uuids:
- workflow          Workflow
- workflow step     WorkflowStep
- workflow input    WorkflowInput
- workflow output   WorkflowOutput
- tool              Tool
- tool input        CommandComponent
- tool output       CommandComponent





upgraded refgenconf (pip install --upgrade refgenconf) because installed version brokey




Workflow:
    metadata: WorkflowMetadata
    steps: dict[int, GalaxyWorkflowStep]
    inputs: list[WorkflowInput]
    outputs: list[WorkflowOutput]
    steps_inputs_uuid_map: dict[str, str]

    def get_input(self, uuid: str) -> Optional[WorkflowInput]:



Step:
    metadata: StepMetadata
    input_register: StepInputRegister
    output_register: StepOutputRegister
    tool: Optional[Tool]        (ToolStep)
    values: InputValueRegister  (ToolStep)

    def get_input(self, query_name: str) -> Optional[StepInput]:
    def get_input(self, uuid: str) -> Optional[StepInput]:




Tool:
    metadata: ToolXMLMetadata
    xmlcmdstr: CommandString
    gxparam_register: InputParamRegister
    inputs: list[CommandComponent]
    outputs: list[CommandComponent]
    container: Optional[Container]
    base_command: list[str]










TAGS
during tool parsing:
- tool_name
- tool_component

workflow startup
- create/refresh all workflow stuff
- create/refresh all tool stuff

tool startup
- create tool_name, tool_component json if not exists 


{
    fastqc: {
        
    }



####################################################
please set all values in 'configfile.txt'
please .........
####################################################





configfile.txt/yml/json
- INPUT_READS1
- INPUT_READS2
- INPUT_READS3
- FASTQC_MODE


step1.py
from configfile import INPUT_READS1, FASTQC_MODE



simple_workflow
    - workflow.py
    - steps
        - step1.py
        - step2.py
    - tools
        - fastqc.py
        - quast.py
        

workflow.py

from steps/step1.py import STEP1_FASTQC

notes

inputs = []


w.input(
        "STEP1_FASTQC",
        File,
        doc="Forward reads"
)
w.input(
        "STEP2_FASTQC",
        File,
        doc="Forward reads"
)

STEP2_FASTQC = aoskd


CWL:

config.json
    {
        STEP1_FASTQC:__runtime__,
        STEP1_FASTQC:__runtime__,
        STEP1_FASTQC:__runtime__,
    }



# please set the following
STEP1_INPUT1        = # SETME
STEP1_OUTDIR        = # SETME
STEP1_CONTAMINANTS  = # SETME
STEP1_ADAPTERS      = # SETME
STEP1_LIMITS        = # SETME
STEP1_F_FILE        = # SETME



steps [
    STEP1_FASTQC,
    STEP2_FASTQC,
    STEP3_MULTIQC
]

outputs = []

workflow.py

    # input values
    INPUT_READS1 = #SET
    INPUT_READS2 = #SET
    INPUT_READS3 = #SET
    FASTQC_MODE = #SET



    # line 102: step2 fastqc
    input1=RUNTIMEVALUE,
    outdir=RUNTIMEVALUE,
    contaminants=RUNTIMEVALUE,    # this is a tabular file. [TOOL INPUT DOCSTRING]
    adapters=RUNTIMEVALUE,
    limits=RUNTIMEVALUE,
    f_file=RUNTIMEVALUE





# NOTE ON THESE INPUTS
# contaminants
# this is a tabular file. [TOOL INPUT DOCSTRING] 


# please set the following
STEP1_INPUT1        = # SETME
STEP1_OUTDIR        = # SETME
STEP1_CONTAMINANTS  = # SETME
STEP1_ADAPTERS      = # SETME
STEP1_LIMITS        = # SETME
STEP1_F_FILE        = # SETME

w.step(
    "step4_fastqc",
    fastqc(
            quiet=False,
            extract=False,
            nogroup=false,
            input1=STEP1_INPUT1,
            outdir=STEP1_OUTDIR,
            contaminants=STEP1_CONTAMINANTS,    # this is a tabular file. [TOOL INPUT DOCSTRING]
            adapters=STEP1_ADAPTERS,
            limits=STEP1_LIMITS,
            kmers=7,
            f_file=STEP1_F_FILE,
    )
)










































CommandString
- represents a command line string
- holds list of statements
- main: main CommandStatement which looks like tool execution
- preprocessing and postprocessing are also held (list of CommandStatements)

DynamicCommandStatement
- represents a single command in a command line string
- allows us to dynamically get a realised ExecutionPath(concrete values for stuff)
- also allows us to get the unrealised values where all gxparam references are still embedded

ExecutionPath
- a concrete, realised DynamicCommandStatement
- ie abricate --input1 myfasta.fq --no-header --min-cov=100
- this is different to DynamicCommandStatement because it's values have been realised. see below:
DynamicCommandStatement: abricate $sample_name $noheader --minid=$adv.minid --db=$adv.db 
ExecutionPath: abricate $sample_name --no-header --minid=80 --db=card 
in the ExecutionPath, values have become realised, rather than variables if possible. $sample_name is an env_var, so cannot yet inject it. maybe later

GreedyEPathAnnotator
- accepts an ExecutionPath and annotates it with components 
- Positionals, Flags, Options

Command
- created from parsing multiple ExecutionPaths from multiple DynamicCommandStatements
- idea is that the main tool execution DynamicCommandStatement is expanded into ExecutionPaths, then each updates our understanding of the Command
- at the end, any outputs we didn't find are added




A note on outputs:

Outputs
- RedirectOutput
- InputOutput
- WildcardOutput

RedirectOutputs and InputOutputs are good because we know whether they are generated by the
main tool statement. 
If we have a main tool statement AND NO posprocessing statements, all WildcardOutputs are from the main tool
being executed.
If we have a main tool statement AND postprocessing statements, a WildcardOutput may have been generated
from the main tool statement, or from a postprocessing statement. 
in the above case, inform the user that we don't know if this output was created by the main
tool execution step or the postprocessing step. 
A solution is to carry all files created, RedirectOutputs and InputOutputs from the the preprocessing and main steps into the postprocessing step. this ensures all outputs thus created are present, and any which are created as part of postprocessing will also now be available. Unsure whether we should do this by default or via user CLI setting. 



fill_template() in __build_command_line() actually tempates galaxy tool
(src/galaxy/tools/evaluation.py)
read galaxy src for logging pattern


WORKFLOWS
- read galaxy source to uncover how connections are created



- workflow skeleton





Questions
- what to do when the tool version doesnt match the main requirement version?


v0.1 unsupported -----------------------------------------------------------------

- general
    - different versions of same tool
        - container cache is holding version number at least 

- command string
    - \$_GALAXY_JOB_TMP_DIR (no idea what to do)
    - for loops
    - multiline bash blocks
    - MULTIPLE EXPANDED FORMS (filtlong) why?
    - Multiline strings (see abyss-pe.xml)
        #if $lib_repeat
            lib='
            #for $i in range(len($lib_repeat))
                lib$i
            #end for
            '
    - INDICIES (bwa-mem)
    - -o first${ext}
    - proper reconstruction of command order using cheetah command logic
    - tools with multiple commands (only first is interpreted at the moment) (bwa.xml is good example)
        - do I need to consider '&&' after the first '>' / '|' ? 
    - python logic contained in between {} (currently just causes errors):
        -t ${ ','.join( [ "'%s'" %  $x for $x in $treatment.input_treatment_file] ) }



- param features 
    - non-unique param / output names 
        - ie param1.name = param2.name but param1.gx_var !=
          param2.gx_var
    - builtin datasets
    - repeat tags
        - repeat tags are far too complex
        - will attempt to update tool with this understanding through galaxy templating engine and runtime settings
    - ignored complex or rare features 
        - data->change_format
        - data->actions
        - data->default_identifier_source
        - discover_datasets->recurse
    - tools with multiple commands
        - each '|' or '&&' or ';' could represent a different command that could be run as a step
        - in these cases, the single tool xml can be split into multiple janis tool definitions
        - first, split the command string into each individual command
        - identify the particular command which seems to be main one (ie bwa in bwa.xml, even though it has a number of samtools commands)
        - when executing workflows, it could be known that the tool xml was split into (4) janis tools
        - the step could then run as sort-of a sub-workflow
        - when splitting a tool xml into multiple tool definitions: 
            - first check if a tool def exists 
            - if so, add any newly discovered params / outputs / command components
            - else, just create a new one from scratch
            - this approach is already employed by the main workflow specs incl janis. 
            - sometimes even something simple like 'tr a-z A-Z < file.txt' is run as an individual step in a vanilla ubuntu container


General Notes --------------------------------------------------------------------

- tool versions
    - the version in the tool xml may not actually be the same as the requirement
    - merely represents the information in the wrapper, not the actual version of the tool that the container executes
    - for some tools, there are multiple versions that appear to be the same. for bwa-mem (bwa) we have:
        - 0.7.17
        - v0.7.17
        - v0.7.17-3-deb (debian?)
        
- tool name
    - both the tool_id and tool_name in the xml are unreliable.
    - tool name is taken as tool_id.replace('-', '_').lower()
    - this isn't necessarily the same name as the main requirement being executed
    - again, simply gives a reasonable name specified from the tool xml
    - this allows us to then parse gx workflows and actually understand what tool definition the janis should load. 

- command line
    - option or flag?
        coords2clnt.py --threeprime '$alignment_coordinates' > '$crosslinking_coordinates'    
        ~ is the $alignment_coordinates standalone, or an opt with --threeprime as prefix? its standalone in this case but really looks like an opt. --threeprime is a flag
    
    - anything line beginning with with #silent is removed
    - multiline strings are handled so that if a line has an unpaired quote or double quote it is replaced with MULTILINE_STRING
    - circos: bug? ${reference_genome.ref.input_lengths}'


end ------------------------------------------------------------------------------



SUPPORTED REDIRECTION
0 is stdin, 1 is stdout, and 2 is stderr
'&amp;' is also accepted as '&'
'>>' is also accepted as '>'

STDOUT to file (often used for outputs) (337 tools)
    construct           > file
    galaxy occurance    > '$report'   (report has format="tabular")
    janis solution      ToolOutput("report", Tabular, Stdout(optional=False))

STDERR to file (4 tools)
    construct           2>&1 > file
    galaxy occurance    2>&1 > $file_stderr
    janis solution      ToolOutput("file_stderr", Textfile, Stdout(optional=False))

branch/split STDOUT to a file (usually for logs) (142 tools)
    construct           | tee file
    galaxy occurance    '$out_result' | tee '$out_log'  (out_log has format="txt")
    janis solution      ToolOutput("report", Textfile, Stdout(optional=False))
    
    does janis support this? 
        - should be fine in situations where '| tee' effectively replaces '>'
        - ie when whatever gets passed to '| tee' is the final stdout of the tool step
        - what if the tool continues and performs other actions? then goes on to collect stdout at a later time point? 

branch/split STDERR & STDOUT to a file (usually for logs) (3 tools)
    construct           |& tee file (short for `2>&1 | tee file`)
    galaxy occurance    --verbose 3 |& tee '$out_log'
    janis solution      ToolOutput("out_log", Datatype, Stdout(optional=False))

STDIN
    construct           < infile
    galaxy occurance    datamash reverse < $in_file > $out_file
    janis solution      ToolInput("in_file", Datatype, prefix="<" etc)

STDOUT to STDERR (4 tools)
    construct           1>&2 
    galaxy occurance    echo "Reporting ... single input sample" 1>&2; exit 1 
    janis solution      ignore. handled by workflow manager 

STDERR to STDOUT (50 tools)
    construct           2>&1
    galaxy occurance    '$inputfile' '$output' 2>&1
    janis solution      ignore. handled by workflow manager 

something to STDERR (usually a quoted string or logfile) (19 tools)
    construct           >&2
    galaxy occurance    >&2 "unknown extension $in.input.ext"
    janis solution      ignore. handled by workflow manager 


Note
If |& is used, the standard error of command is connected to command2’s standard input through the pipe; it is shorthand for 2>&1 |
    |& tee -a '$out_log'
    2>&1 | tee -a '$out_log'

Bin something
    /dev/null


Params
    the galaxy params are more like suggestions than rules
    galaxy params should be parsed as-is
    at postprocessing stage should be remapped into final understanding of underlying tool


AT postprocess stage
    bool params:
        - truevalue and falsevalue are 


from non-galaxy command string
--augustus=true

have a registry
know that
    --augustus=true is key-val pair
    --augustus= can be set from $use_augustus boolean gx param
    can simply look at $use_augustus and confirm that either truevalue or falsevalue == '--augustus=true'
    can then set that attribute when translating external -> galaxy 


SELECT PARAMS

list of flags (one may be blank)

    (bedtools bamtobed)
    <option value="">Create a 6-column BED file</option>
    <option value="-bed12">Create a full, 12-column "blocked" BED  file</option>
    <option value="-bedpe">Create a paired-end, BEDPE format</option>

list of options

    (bedtools genomecoveragebed)
    <option value="">both strands combined</option>
    <option value="-strand +">positive strand only</option>
    <option value="-strand -">negative strand only</option>

list of kv pairs

    (lastz)
    <option value="--strand=both" selected="True">Both</option>
    <option value="--strand=plus">Plus</option>
    <option value="--strand=minus">Minus</option>

a blend

    (lastz)
    <option value="--transition" selected="true">One</option>
    <option value="--transition=2">Two</option>
    <option value="--notransition">None</option>


if the param is select
    check that each option is "" or starts with "-"
    if true:
        inject all the options directly into the list of command words





BOOLEAN PARAMS

SINGLE VALUE

single value but its assigned to falsevalue

    (mothur pairwise.seqs)
    truevalue="" falsevalue="countends=false," 

single value and its spaced kv pair 
NOTE: this is for a configfile! 

    (circos macros)
    truevalue="flow = continue"
    falsevalue=""


TWO DIFFERENT VALUES

    (allegro)
    truevalue="on"
    falsevalue="off"

    (bcftools_query)
    truevalue="True"
    falsevalue="False"

    (art/macros)
    truevalue="centimorgan"
    falsevalue="recombination"

    (augustus)
    truevalue="1"
    falsevalue="0" 

involving flags

    (kofamscan)
    truevalue="--report-unannotated"
    falsevalue="--no-report-unannotated" 

kv_pairs yes/no

    (minimap2)
    truevalue="--splice-flank=yes" 
    falsevalue="--splice-flank=no" 

single option, two values

    (velvetg)
    truevalue="-read_trkg yes" 
    falsevalue="-read_trkg no"




skip conditions
at least 1 of [truevalue, falsevalue] == ""

else
expand every option (select param), truevalue/falsevalue and try to interpret as 









flag-list select params:



Richard:
    - how do you want to handle select params encompassing multiple flags?


Tools with unsupported features

For loops
    - genetrack
    - annotateBed


cutadapt generally super fucked

Macro parsing not working as expected! Fastp




Current thoughts:
- optionality for arguments is not possible to identify. can supply defaults in most cases as workaround. 


Command parsing
    - #include not supported
    - multiline bash blocks not supported (about 12 tools)
    - currently throwing out bash conditionals
    - should add bash aliases - 'export AUGUSTUS_CONFIG_PATH=`pwd`/augustus_dir/'


Non-supported features
    - for loop statements
    - aliases with anything except '#set x = y' structure


What the hell is bcftools_cnv.xml doing with mv $outputs? 

Command parsing

what is the main goal here? 
- options (and their arguments)
- identify the base command word
- positional arguments

   

COMMAND DATASTRUCTURES

aliases
{
    source [str]:    alias [Alias]
                    {
                        alias.source: str
                        alias.dest: str
                        alias.text: str
                        alias.instruction: str
                    }
}

env_vars - list of strings (each string is varname)
{
    '$AUGUSTUS_CONFIG_PATH',
}

params - gx_var is stripped to minimal form: $var
{
    param.gx_var [str]: param [Param]
                        {
                            param.name: str
                            param.gx_var: str
                            param.janis_var: str
                            param.galaxy_type: str
                        }
}

command_words
[
    cmdword [CommandWord]
    {
        cmdword.text = text
        cmdword.command_num = command_num
        cmdword.in_loop = False
        cmdword.in_conditional = False
    },

    
]








Datatypes 

UnionType 
 - never used
 - single datatype is selected instead 

Optional
 - if optional="true" in param
 - if appears only in conditional line
 - need to add in conditional block

Array
 - only if 
    - data_collection param
    - data param with multiple="True"
    - collection output
    - output with '*' or '.+' in 'pattern'












should I have a list of unhandled scenarios for runtime warnings? yes.

